{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "733fca30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] using sitemap https://www.haldirams.com/media/sitemap/sitemap.xml\n",
      "[warn] https://www.haldirams.com/corporate-gifting.html skipped – 404 Client Error: Not Found for url: https://www.haldirams.com/corporate-gifting.html\n",
      "✅ Saved 199 items → C:\\Users\\prmsr\\OneDrive - iitkgp.ac.in\\Desktop\\Zomato\\haldirams_menu.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "scrape_haldirams_menu.py  ·  May-2025\n",
    "Outputs haldirams_menu.json with the exact McDonald's-style schema.\n",
    "\n",
    "Robustness features\n",
    "───────────────────\n",
    "• Tries every sitemap URL listed in robots.txt *and* a fallback list.\n",
    "• If all sitemaps fail (404/403), crawls the main-nav category links.\n",
    "• Honors robots.txt, polite delays, normal desktop User-Agent.\n",
    "• Flags vegetarian/vegan, spicy, allergen keywords.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import json, re, time, random, io\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Iterable\n",
    "from urllib import robotparser\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "\n",
    "# ───────── CONFIG ─────────\n",
    "ROOT  = \"https://www.haldirams.com\"\n",
    "UA    = (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "         \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "         \"Chrome/124.0.0.0 Safari/537.36\")\n",
    "MAX_ITEMS  = 200\n",
    "DELAY_SEC  = 1.0\n",
    "OUT_FILE   = Path(\"haldirams_menu.json\")\n",
    "FALLBACK_SITEMAPS = [\n",
    "    \"/sitemap.xml\",\n",
    "    \"/sitemap_index.xml\",\n",
    "    \"/sitemap/sitemap.xml\",\n",
    "    \"/media/sitemap/sitemap.xml\",\n",
    "]\n",
    "# ──────────────────────────\n",
    "\n",
    "\n",
    "# ---------- networking helpers ----------\n",
    "def fetch(url: str, binary=False) -> str | bytes:\n",
    "    hdr = {\n",
    "        \"User-Agent\": UA,\n",
    "        \"Accept\": \"*/*\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    }\n",
    "    r = requests.get(url, headers=hdr, timeout=30, allow_redirects=True)\n",
    "    r.raise_for_status()\n",
    "    return r.content if binary else r.text\n",
    "\n",
    "\n",
    "def robots_allows(path: str) -> bool:\n",
    "    rp = robotparser.RobotFileParser()\n",
    "    rp.set_url(urljoin(ROOT, \"/robots.txt\"))\n",
    "    try:\n",
    "        rp.read()\n",
    "        return rp.can_fetch(UA, path)\n",
    "    except Exception:\n",
    "        return True\n",
    "\n",
    "\n",
    "# ---------- sitemap discovery ----------\n",
    "def discover_sitemap_urls() -> Iterable[str]:\n",
    "    # read robots.txt\n",
    "    try:\n",
    "        robots = fetch(urljoin(ROOT, \"/robots.txt\"))\n",
    "        for line in robots.splitlines():\n",
    "            if line.lower().startswith(\"sitemap:\"):\n",
    "                yield line.split(\":\", 1)[1].strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "    # fallbacks\n",
    "    for path in FALLBACK_SITEMAPS:\n",
    "        yield urljoin(ROOT, path)\n",
    "\n",
    "\n",
    "def get_first_live_sitemap() -> str | None:\n",
    "    for url in discover_sitemap_urls():\n",
    "        try:\n",
    "            fetch(url, binary=True)       # just to test 200 \n",
    "            return url\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_product_urls_from_sitemap(xml_bytes: bytes) -> List[str]:\n",
    "    ns = {\"sm\": \"http://www.sitemaps.org/schemas/sitemap/0.9\"}\n",
    "    doc = etree.fromstring(xml_bytes)\n",
    "    locs = [e.text for e in doc.xpath(\"//sm:url/sm:loc\", namespaces=ns)]\n",
    "    return [\n",
    "        u for u in locs\n",
    "        if u.endswith(\".html\")\n",
    "        and \"/blog/\" not in u\n",
    "        and \"/category/\" not in u\n",
    "    ]\n",
    "\n",
    "\n",
    "# ---------- HTML fallback (nav crawl) ----\n",
    "def nav_category_links() -> List[str]:\n",
    "    soup = BeautifulSoup(fetch(ROOT), \"html.parser\")\n",
    "    links = []\n",
    "    for a in soup.select(\"nav a[href]\"):\n",
    "        href = a[\"href\"]\n",
    "        if href.endswith(\".html\") and \"javascript:\" not in href:\n",
    "            full = href if href.startswith(\"http\") else urljoin(ROOT, href)\n",
    "            links.append(full)\n",
    "    # dedupe preserve order\n",
    "    seen, out = set(), []\n",
    "    for link in links:\n",
    "        if link not in seen:\n",
    "            seen.add(link)\n",
    "            out.append(link)\n",
    "    return out\n",
    "\n",
    "\n",
    "def product_links_from_category(cat_url: str) -> List[str]:\n",
    "    soup = BeautifulSoup(fetch(cat_url), \"html.parser\")\n",
    "    out = []\n",
    "    for a in soup.select(\"a.product-item-link[href]\"):\n",
    "        href = a[\"href\"]\n",
    "        if href.startswith(\"http\"):\n",
    "            out.append(href)\n",
    "        else:\n",
    "            out.append(urljoin(ROOT, href))\n",
    "    return out\n",
    "\n",
    "\n",
    "# ---------- feature flags ---------------\n",
    "def special_features(text: str) -> List[str]:\n",
    "    low = text.lower()\n",
    "    feats = []\n",
    "    if any(k in low for k in (\"veg\", \"vegan\", \"vegetarian\")):\n",
    "        feats.append(\"Vegetarian / Vegan option\")\n",
    "    if \"spicy\" in low or \"chilli\" in low:\n",
    "        feats.append(\"Spicy\")\n",
    "    if \"allergen\" in low or (\"contains\" in low and \"nuts\" in low):\n",
    "        feats.append(\"Contains allergen information\")\n",
    "    return feats\n",
    "\n",
    "\n",
    "# ---------- product parser --------------\n",
    "def parse_product(url: str) -> Dict:\n",
    "    soup = BeautifulSoup(fetch(url), \"html.parser\")\n",
    "\n",
    "    # name\n",
    "    h1 = soup.find(\"h1\")\n",
    "    name = h1.get_text(strip=True) if h1 else \"\"\n",
    "    if not name:\n",
    "        ogt = soup.find(\"meta\", property=\"og:title\")\n",
    "        name = ogt[\"content\"].strip() if ogt and ogt.get(\"content\") else url.rsplit(\"/\", 1)[-1]\n",
    "\n",
    "    # description\n",
    "    box = soup.find(\"div\", class_=\"product attribute overview\")\n",
    "    desc = box.get_text(\" \", strip=True) if box else \"\"\n",
    "    if not desc:\n",
    "        ogd = soup.find(\"meta\", property=\"og:description\")\n",
    "        desc = ogd[\"content\"].strip() if ogd and ogd.get(\"content\") else \"Description not available\"\n",
    "    else:\n",
    "        desc = re.sub(r\"\\s+\", \" \", desc)\n",
    "\n",
    "    # category breadcrumb\n",
    "    cat = \"Uncategorised\"\n",
    "    bc = soup.select(\"ul.breadcrumbs li\")\n",
    "    if len(bc) >= 2:\n",
    "        cat = bc[1].get_text(strip=True)\n",
    "\n",
    "    return {\n",
    "        \"item_name\": name,\n",
    "        \"description\": desc,\n",
    "        \"special_features\": special_features(desc) or None,\n",
    "        \"product_url\": url,\n",
    "        \"price\": None,\n",
    "        \"category\": cat,\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- main workflow ---------------\n",
    "def scrape_haldirams() -> Dict:\n",
    "    live_sitemap = get_first_live_sitemap()\n",
    "    product_urls: List[str] = []\n",
    "\n",
    "    if live_sitemap:\n",
    "        print(f\"[info] using sitemap {live_sitemap}\")\n",
    "        xml = fetch(live_sitemap, binary=True)\n",
    "        product_urls = extract_product_urls_from_sitemap(xml)\n",
    "    else:\n",
    "        print(\"[warn] no live sitemap – falling back to nav crawl\")\n",
    "        cats = nav_category_links()\n",
    "        for c in cats:\n",
    "            product_urls.extend(product_links_from_category(c))\n",
    "            time.sleep(0.6)\n",
    "\n",
    "    # dedupe & trim\n",
    "    seen, uniq = set(), []\n",
    "    for u in product_urls:\n",
    "        if u not in seen:\n",
    "            seen.add(u)\n",
    "            uniq.append(u)\n",
    "        if len(uniq) >= MAX_ITEMS:\n",
    "            break\n",
    "\n",
    "    items: List[Dict] = []\n",
    "    for url in uniq:\n",
    "        path = urlparse(url).path\n",
    "        if not robots_allows(path):\n",
    "            continue\n",
    "        try:\n",
    "            items.append(parse_product(url))\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] {url} skipped – {e}\")\n",
    "        time.sleep(DELAY_SEC + random.uniform(0, 0.4))\n",
    "        if len(items) >= MAX_ITEMS:\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"restaurant_name\": \"Haldiram's\",\n",
    "        \"location\": \"India\",\n",
    "        \"opening_hours\": None,\n",
    "        \"contact_info\": None,\n",
    "        \"scrape_source\": live_sitemap or \"HTML-navigation fallback\",\n",
    "        \"item_count\": len(items),\n",
    "        \"items\": items,\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- save ------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    data = scrape_haldirams()\n",
    "    OUT_FILE.write_text(\n",
    "        json.dumps(data, indent=2, ensure_ascii=False),\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "    print(f\"✅ Saved {data['item_count']} items → {OUT_FILE.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736e7fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip freeze > requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
