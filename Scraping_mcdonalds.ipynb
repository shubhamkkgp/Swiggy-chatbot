{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "812eec40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 93 items → C:\\Users\\prmsr\\OneDrive - iitkgp.ac.in\\Desktop\\Zomato\\mcd_menu.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "scrape_mcd_menu.py\n",
    "BeautifulSoup-based scraper for McDonald’s US menu ─ max 200 items.\n",
    "Outputs: mcd_menu.json (UTF-8, pretty-printed)\n",
    "\"\"\"\n",
    "\n",
    "import json, re, time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from urllib import robotparser\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ────────────────────────── Config ──────────────────────────\n",
    "ROOT_URL       = \"https://www.mcdonalds.com\"\n",
    "MENU_URL       = f\"{ROOT_URL}/us/en-us/full-menu.html\"\n",
    "LOCATION       = \"USA\"\n",
    "USER_AGENT     = \"Mozilla/5.0 (compatible; mcd-scraper/1.1)\"\n",
    "MAX_ITEMS      = 200\n",
    "REQUEST_DELAY  = 1.2                       # seconds between item pages\n",
    "OUT_FILE       = Path(\"mcd_menu.json\")\n",
    "GENERIC_PREFIX = \"our terms and conditions\"  # phrase signalling legal banner\n",
    "# ────────────────────────────────────────────────────────────\n",
    "\n",
    "\n",
    "# ---------- helpers ----------------------------------------------------------\n",
    "def allowed_by_robots(url: str, ua: str = USER_AGENT) -> bool:\n",
    "    \"\"\"Return True if URL may be fetched according to robots.txt.\"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    robots_url = f\"{parsed.scheme}://{parsed.netloc}/robots.txt\"\n",
    "    rp = robotparser.RobotFileParser()\n",
    "    rp.set_url(robots_url)\n",
    "    try:\n",
    "        rp.read()\n",
    "    except Exception:\n",
    "        return False\n",
    "    return rp.can_fetch(ua, url)\n",
    "\n",
    "\n",
    "def clean(txt: str | None) -> str:\n",
    "    \"\"\"Collapse whitespace & trim.\"\"\"\n",
    "    return re.sub(r\"\\s+\", \" \", txt or \"\").strip()\n",
    "\n",
    "\n",
    "def fetch(url: str) -> BeautifulSoup:\n",
    "    \"\"\"HTTP-GET a page and return BeautifulSoup object.\"\"\"\n",
    "    headers = {\"User-Agent\": USER_AGENT}\n",
    "    resp = requests.get(url, headers=headers, timeout=15)\n",
    "    resp.raise_for_status()\n",
    "    return BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "\n",
    "# ---------- menu-level extraction -------------------------------------------\n",
    "def extract_restaurant_meta(menu_soup: BeautifulSoup) -> Tuple[str | None, str | None]:\n",
    "    \"\"\"Look inside JSON-LD for openingHours / telephone.\"\"\"\n",
    "    hours, phone = None, None\n",
    "    for script in menu_soup.find_all(\"script\", type=\"application/ld+json\"):\n",
    "        try:\n",
    "            data = json.loads(script.string)\n",
    "        except Exception:\n",
    "            continue\n",
    "        # data can be obj or list; unify to list for convenience\n",
    "        objs = data if isinstance(data, list) else [data]\n",
    "        for obj in objs:\n",
    "            if isinstance(obj, dict) and obj.get(\"@type\") == \"Restaurant\":\n",
    "                hours = hours or obj.get(\"openingHours\")\n",
    "                phone = phone or obj.get(\"telephone\")\n",
    "        if hours and phone:\n",
    "            break\n",
    "    return hours, phone\n",
    "\n",
    "\n",
    "def extract_menu_links(menu_soup: BeautifulSoup) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Return list of (category, absolute_link) pairs.\"\"\"\n",
    "    links: List[Tuple[str, str]] = []\n",
    "    for heading in menu_soup.select(\"h2, h3\"):\n",
    "        category = clean(heading.get_text())\n",
    "        sibling = heading.find_next([\"ul\", \"div\"])\n",
    "        if not sibling:\n",
    "            continue\n",
    "        for a in sibling.find_all(\"a\", href=True):\n",
    "            href = a[\"href\"]\n",
    "            if \"/product/\" in href:\n",
    "                links.append((category, urljoin(ROOT_URL, href)))\n",
    "    return links\n",
    "\n",
    "\n",
    "# ---------- product-level extraction ----------------------------------------\n",
    "def special_features_from_text(text: str) -> List[str]:\n",
    "    text_lower = text.lower()\n",
    "    features: List[str] = []\n",
    "\n",
    "    if \"vegetarian\" in text_lower or \"vegan\" in text_lower:\n",
    "        features.append(\"Vegetarian / Vegan option\")\n",
    "    if \"spicy\" in text_lower:\n",
    "        features.append(\"Spicy\")\n",
    "    allergen_match = re.search(r\"allergens?:?\\s*(.+?)(?:\\.\\s|$)\", text_lower, re.I)\n",
    "    if allergen_match:\n",
    "        features.append(\"Allergens: \" + clean(allergen_match.group(1)))\n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_product_details(product_url: str) -> Dict:\n",
    "    \"\"\"Return dict with item_name, description, special_features.\"\"\"\n",
    "    soup = fetch(product_url)\n",
    "\n",
    "    # ---------- item name ----------\n",
    "    name_tag = soup.find([\"h1\", \"h2\"])\n",
    "    item_name = clean(name_tag.get_text()) if name_tag else \"Unnamed item\"\n",
    "\n",
    "    # ---------- description ----------\n",
    "    desc = \"\"\n",
    "    #   Try meta description first\n",
    "    meta_desc = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "    if meta_desc and meta_desc.get(\"content\"):\n",
    "        desc = clean(meta_desc[\"content\"])\n",
    "    #   Fallback: first <p>\n",
    "    if not desc:\n",
    "        p = soup.find(\"p\")\n",
    "        desc = clean(p.get_text()) if p else \"\"\n",
    "\n",
    "    if not desc or desc.lower().startswith(GENERIC_PREFIX):\n",
    "        desc = \"Description not available\"\n",
    "\n",
    "    # ---------- special features ----------\n",
    "    features = special_features_from_text(soup.get_text(\" \", strip=True))\n",
    "\n",
    "    return {\n",
    "        \"item_name\": item_name,\n",
    "        \"description\": desc,\n",
    "        \"special_features\": features or None,\n",
    "        \"product_url\": product_url,\n",
    "        \"price\": None  # price not public on full-menu pages\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- main workflow ----------------------------------------------------\n",
    "def scrape_mcd_menu() -> Dict:\n",
    "    if not allowed_by_robots(MENU_URL):\n",
    "        raise RuntimeError(\"Blocked by robots.txt – cannot scrape.\")\n",
    "\n",
    "    menu_soup = fetch(MENU_URL)\n",
    "    opening_hours, telephone = extract_restaurant_meta(menu_soup)\n",
    "\n",
    "    all_links = extract_menu_links(menu_soup)\n",
    "\n",
    "    results: List[Dict] = []\n",
    "    seen: set[str] = set()\n",
    "\n",
    "    for category, url in all_links:\n",
    "        if len(results) >= MAX_ITEMS:\n",
    "            break\n",
    "        if url in seen:\n",
    "            continue\n",
    "        seen.add(url)\n",
    "        try:\n",
    "            details = extract_product_details(url)\n",
    "        except Exception as exc:\n",
    "            print(f\"[warn] {url} skipped ({exc})\")\n",
    "            continue\n",
    "        details[\"category\"] = category\n",
    "        results.append(details)\n",
    "        time.sleep(REQUEST_DELAY)\n",
    "\n",
    "    return {\n",
    "        \"restaurant_name\": \"McDonald's\",\n",
    "        \"location\": LOCATION,\n",
    "        \"opening_hours\": opening_hours,\n",
    "        \"contact_info\": telephone,\n",
    "        \"scrape_source\": MENU_URL,\n",
    "        \"item_count\": len(results),\n",
    "        \"items\": results,\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = scrape_mcd_menu()\n",
    "    OUT_FILE.write_text(\n",
    "        json.dumps(data, indent=2, ensure_ascii=False),\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "    print(f\"✅ Saved {data['item_count']} items → {OUT_FILE.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
